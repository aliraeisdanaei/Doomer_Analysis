{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = re.compile(r\"[\\n,!\\?\\’'\\+:\\\"\\.\\$&@#/\\(\\)\\[\\]\\|\\{\\}]\")\n",
    "nonwords = re.compile('^[^a-zA-Z0-9]+$')\n",
    "links = re.compile(r\"https\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df, col_to_clean: str): \n",
    "    df[col_to_clean] = df[col_to_clean].str.lower().replace('\\n','',regex=True).str.replace('’',\"'\",regex=True).str.replace(punc,'',regex=True).str.strip()\n",
    "    df = df[~df[col_to_clean].str.contains(links)]\n",
    "    df = df[~df[col_to_clean].str.contains(nonwords)]\n",
    "    df = df[df[col_to_clean]!='']\n",
    "    return df\n",
    "\n",
    "def tokinize(df, col_to_tokenize: str):\n",
    "    df['tokenized'] = df[col_to_tokenize].str.split(' ')\n",
    "    return df\n",
    "\n",
    "def get_freqs(df, col_tok='tokenized'):\n",
    "    df = df.explode(col_tok)\n",
    "    freqs = df[col_tok].value_counts().to_dict()\n",
    "    STOPWORDS.update({punc.sub('',x) for x in STOPWORDS})\n",
    "    new_freqs = {word:freq for (word,freq) in freqs.items() if word not in STOPWORDS}\n",
    "    return new_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_df(json_filename):\n",
    "    df = pd.read_json(json_filename, lines=True)\n",
    "    # changing the type col\n",
    "    df = df.rename({\"_type\": \"type\"}, axis=\"columns\")\n",
    "    df = df.replace(to_replace=\"snscrape.modules.reddit.Submission\", value=\"post\")\n",
    "    df = df.replace(to_replace=\"snscrape.modules.reddit.Comment\", value=\"comment\")\n",
    "    # change to posts only\n",
    "    df_posts = df[df[\"type\"] == \"post\"]\n",
    "    df_posts = df_posts.dropna(subset=[\"selftext\"])\n",
    "    df_posts = df_posts.drop([\"body\"], axis=1)\n",
    "    df_posts = df_posts.drop([\"type\", \"parentId\", \"subreddit\", \"link\"], axis=1)\n",
    "    df_posts = df_posts.reset_index()\n",
    "\n",
    "    return df, df_posts\n",
    "\n",
    "def apply_text_len(df, col_text='selftext', minimum_length=500, visualise_lengths=False):\n",
    "    df['text_len'] = df[col_text].apply(len)\n",
    "    if(visualise_lengths):\n",
    "        plt2 = df['text_len'].plot(kind='kde',\n",
    "            title=\"Distribution of Comment Length\",\n",
    "            xlabel='Comment Length',\n",
    "            xlim=0)\n",
    "    df = df[df[\"text_len\"] >= minimum_length]\n",
    "\n",
    "    df = clean_text(df, col_text)\n",
    "    return df\n",
    "\n",
    "def draw_wordcloud(freqs):\n",
    "    wordcloud = WordCloud()\n",
    "    wordcloud.generate_from_frequencies(frequencies=freqs)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising the Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, doomer_posts = get_post_df(\"Data/r_doomer_scraped.json\")\n",
    "_, lonely1_posts = get_post_df(\"Data/r_lonely_scraped.json\")\n",
    "_, lonely2_posts = get_post_df(\"Data/r_loneliness_scraped.json\")\n",
    "_, depr_posts = get_post_df(\"Data/r_depression_scraped.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doomer_posts = apply_text_len(doomer_posts)\n",
    "lonely1_posts = apply_text_len(lonely1_posts)\n",
    "lonely2_posts = apply_text_len(lonely2_posts)\n",
    "depr_posts = apply_text_len(depr_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2921\n",
      "4860\n",
      "2754\n",
      "8811\n"
     ]
    }
   ],
   "source": [
    "print(len(doomer_posts))\n",
    "print(len(lonely1_posts))\n",
    "print(len(lonely2_posts))\n",
    "print(len(depr_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of doomer: 1275.1535145888595\n",
      "Average length of lonely1: 1228.0242533573862\n",
      "Average length of lonely2: 1489.4160427807487\n",
      "Average length of depr: 1336.8282794624015\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average length of doomer: {doomer_posts['text_len'].mean()}\")\n",
    "print(f\"Average length of lonely1: {lonely1_posts['text_len'].mean()}\")\n",
    "print(f\"Average length of lonely2: {lonely2_posts['text_len'].mean()}\")\n",
    "print(f\"Average length of depr: {depr_posts['text_len'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doomer\n",
      "Earliest post: 2019-04-22T10:51:08+00:00\n",
      "Oldest post: 2022-11-15T16:58:27+00:00\n",
      "Lonely1\n",
      "Earliest post: 2022-09-13T03:50:53+00:00\n",
      "Oldest post: 2022-11-17T00:56:25+00:00\n",
      "Lonely2\n",
      "Earliest post: 2011-11-09T07:06:30+00:00\n",
      "Oldest post: 2022-11-15T03:53:57+00:00\n",
      "Depr\n",
      "Earliest post: 2022-02-28T23:44:13+00:00\n",
      "Oldest post: 2022-11-15T16:33:02+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Doomer\")\n",
    "print(f\"Earliest post: {doomer_posts['created'].min()}\")\n",
    "print(f\"Oldest post: {doomer_posts['created'].max()}\")\n",
    "\n",
    "print(\"Lonely1\")\n",
    "print(f\"Earliest post: {lonely1_posts['created'].min()}\")\n",
    "print(f\"Oldest post: {lonely1_posts['created'].max()}\")\n",
    "\n",
    "print(\"Lonely2\")\n",
    "print(f\"Earliest post: {lonely2_posts['created'].min()}\")\n",
    "print(f\"Oldest post: {lonely2_posts['created'].max()}\")\n",
    "\n",
    "print(\"Depr\")\n",
    "print(f\"Earliest post: {depr_posts['created'].min()}\")\n",
    "print(f\"Oldest post: {depr_posts['created'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doomer_users = doomer_posts['author'].unique()\n",
    "lonely1_users = lonely1_posts['author'].unique()\n",
    "lonely2_users = lonely2_posts['author'].unique()\n",
    "depr_users = depr_posts['author'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1811\n",
      "3851\n",
      "2290\n",
      "8332\n"
     ]
    }
   ],
   "source": [
    "print(len(doomer_users))\n",
    "print(len(lonely1_users))\n",
    "print(len(lonely2_users))\n",
    "print(len(depr_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doomer_lonely1_users = np.intersect1d(doomer_users, lonely1_users) \n",
    "doomer_lonely2_users = np.intersect1d(doomer_users, lonely2_users) \n",
    "doomer_depr_users = np.intersect1d(doomer_users, depr_users)\n",
    "\n",
    "lonely1_lonely2_users = np.intersect1d(lonely1_users, lonely2_users) \n",
    "lonely1_depr_users = np.intersect1d(lonely1_users, depr_users) \n",
    "lonely2_depr_users = np.intersect1d(lonely2_users, depr_users) \n",
    "\n",
    "common_users1 = np.intersect1d(doomer_lonely1_users, doomer_depr_users)\n",
    "common_users2 = np.intersect1d(doomer_lonely2_users, doomer_depr_users)\n",
    "common_users = np.intersect1d(common_users1, common_users2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "6\n",
      "14\n",
      "31\n",
      "161\n",
      "25\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(doomer_lonely1_users))\n",
    "print(len(doomer_lonely2_users))\n",
    "print(len(doomer_depr_users))\n",
    "\n",
    "print(len(lonely1_lonely2_users))\n",
    "print(len(lonely1_depr_users))\n",
    "print(len(lonely2_depr_users))\n",
    "\n",
    "print(len(common_users1))\n",
    "print(len(common_users2))\n",
    "print(len(common_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing a Wordlcoud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doomer_posts = tokinize(doomer_posts, 'selftext')\n",
    "lonely1_posts = tokinize(lonely1_posts, 'selftext')\n",
    "lonely2_posts = tokinize(lonely2_posts, 'selftext')\n",
    "depr_posts = tokinize(depr_posts, 'selftext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doomer_freqs = get_freqs(doomer_posts)\n",
    "lonely1_freqs = get_freqs(lonely1_posts)\n",
    "lonely2_freqs = get_freqs(lonely2_posts)\n",
    "depr_freqs = get_freqs(depr_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(doomer_freqs)\n",
    "draw_wordcloud(lonely1_freqs)\n",
    "draw_wordcloud(lonely2_freqs)\n",
    "draw_wordcloud(depr_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a Sample to be Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = 50\n",
    "# doomer_posts.sample(n = num_samples)['selftext'].to_csv('Data/sample50_doomer.txt', sep=' ')\n",
    "# lonely1_posts.sample(n = num_samples)['selftext'].to_csv('Data/sample50_lonely.txt', sep=' ')\n",
    "# lonely2_posts.sample(n = num_samples)['selftext'].to_csv('Data/sample50_loneliness.txt', sep=' ')\n",
    "# depr_posts.sample(n = num_samples)['selftext'].to_csv('Data/sample50_depr.txt', sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analaysing age & gender of the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # age_certain = re.findall(r'\\d{2}[mf]', post)\n",
    "    # age_certain.extend(re.findall(r'\\d{2} year', post))\n",
    "    # age_certain.extend(re.findall(r'\\d{2} male', post))\n",
    "    # age_certain.extend(re.findall(r'\\d{2} female', post))\n",
    "    # age_certain.extend(re.findall(r'\\d{2} man', post))\n",
    "    # age_certain.extend(re.findall(r'\\d{2} woman', post))\n",
    "\n",
    "    # if(len(age_certain) != 0):\n",
    "    #     print(f'age certain: {age_certain}')\n",
    "    \n",
    "    # if(len(age_certain) > 2):\n",
    "    #     print(post)\n",
    "\n",
    "    # age.extend(re.findall(r' (\\d{2}),? ', post))\n",
    "    # age.extend(re.findall(r'\\d{2}[s]', post))\n",
    "    # # if len(age) == 0:\n",
    "    # #     age = re.findall(r'\\((\\d{2,3})\\)', post)\n",
    "    # # print(post + \" --- AGE: \"+ str(set(age)))\n",
    "    # if(len(age) != 0):\n",
    "    #     print(\"AGE: \"+ str(set(age)))\n",
    "    # if(len(age) > 2):\n",
    "    #     print(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_gender(df):\n",
    "    ages = {}\n",
    "    genders = {\"male\": [], \"female\": []}\n",
    "    for post in df['selftext']:\n",
    "        rex = re.findall(r'Age[\\:\\s](\\d{2})', post)\n",
    "        rex.extend(re.findall(\n",
    "            # r'(?!.*((when) | till))' +\n",
    "            r'((i am)|(im))' + \n",
    "            r'\\s*((a)|(turning)|(going to)|(gonna be)|(gonna turn))?' + \n",
    "            r'\\s*(\\d{2})' + \n",
    "            r'\\s*[a-z](year|[mf]|male|female|man|woman)?', \n",
    "            post))\n",
    "\n",
    "        if(len(rex) > 0):\n",
    "            # print(rex)\n",
    "\n",
    "            all_ages = [int(match[-2]) for match in rex]\n",
    "            \n",
    "            if(max(all_ages) - min(all_ages) <= 2):\n",
    "                crnt_age = all_ages[0]\n",
    "            else:\n",
    "                crnt_age = min(all_ages)\n",
    "\n",
    "            try:\n",
    "                ages[crnt_age].append(post)\n",
    "            except KeyError:\n",
    "                ages[crnt_age] = [post]\n",
    "\n",
    "            # if(len(set(all_ages)) > 1):\n",
    "            #     print(crnt_age)\n",
    "            #     print(all_ages)\n",
    "            #     print(rex)\n",
    "            #     print(post)\n",
    "            # print(crnt_age)\n",
    "                \n",
    "            all_genders = {gender[-1] for gender in rex if gender[-1] != 'year' and gender[-1] != ''}\n",
    "            if(len(all_genders) > 0):\n",
    "                # print(all_genders)\n",
    "                gender = all_genders.pop()\n",
    "\n",
    "                if(gender == \"m\" or gender == \"male\" or gender == \"man\"):\n",
    "                    genders[\"male\"].append(post)\n",
    "                else:\n",
    "                    genders[\"female\"].append(post)\n",
    "    return ages, genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject_bad_ages(ages:dict, min_age=9, max_age=65) -> dict:\n",
    "    bad_ages = set()\n",
    "    for age in ages:\n",
    "        if(not (min_age < age and age < max_age)):\n",
    "            bad_ages.add(age)\n",
    "        if(len(ages[age]) <= 1):\n",
    "            bad_ages.add(age)\n",
    "    for bad_age in bad_ages:\n",
    "        ages.pop(bad_age)\n",
    "    return ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_count_avg(ages:dict) -> (int, float):\n",
    "    count = 0\n",
    "    total_ages = 0\n",
    "    for k in ages:\n",
    "        crnt_len = len(ages[k])\n",
    "        count += crnt_len\n",
    "        total_ages += k * crnt_len\n",
    "\n",
    "    return count, total_ages / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Ages Found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Ages Found, Ave. Age\n",
      "(259, 22.07335907335907)\n",
      "(801, 23.578027465667915)\n",
      "(466, 24.107296137339056)\n",
      "(1378, 22.728592162554428)\n"
     ]
    }
   ],
   "source": [
    "doomer_ages, doomer_genders = get_age_gender(doomer_posts)\n",
    "lonely1_ages, lonely1_genders = get_age_gender(lonely1_posts)\n",
    "lonely2_ages, lonely2_genders = get_age_gender(lonely2_posts)\n",
    "depr_ages, depr_genders = get_age_gender(depr_posts)\n",
    "\n",
    "print(\"Number of Ages Found, Ave. Age\")\n",
    "print(calc_count_avg(doomer_ages))\n",
    "print(calc_count_avg(lonely1_ages))\n",
    "print(calc_count_avg(lonely2_ages))\n",
    "print(calc_count_avg(depr_ages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Ages Found, Ave. Age\n",
      "(253, 21.26086956521739)\n",
      "(787, 23.092757306226176)\n",
      "(456, 23.55482456140351)\n",
      "(1368, 22.47295321637427)\n"
     ]
    }
   ],
   "source": [
    "doomer_ages = reject_bad_ages(doomer_ages)\n",
    "lonely1_ages = reject_bad_ages(lonely1_ages)\n",
    "lonely2_ages = reject_bad_ages(lonely2_ages)\n",
    "depr_ages = reject_bad_ages(depr_ages)\n",
    "\n",
    "print(\"Number of Ages Found, Ave. Age\")\n",
    "print(calc_count_avg(doomer_ages))\n",
    "print(calc_count_avg(lonely1_ages))\n",
    "print(calc_count_avg(lonely2_ages))\n",
    "print(calc_count_avg(depr_ages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Genders Found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male, Female\n",
      "5 2\n",
      "10 2\n",
      "5 2\n",
      "14 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Male, Female\")\n",
    "print(len(doomer_genders[\"male\"]), len(doomer_genders[\"female\"]))\n",
    "print(len(lonely1_genders[\"male\"]), len(lonely1_genders[\"female\"]))\n",
    "print(len(lonely2_genders[\"male\"]), len(lonely2_genders[\"female\"]))\n",
    "print(len(depr_genders[\"male\"]), len(depr_genders[\"female\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "doomer_corpus = doomer_posts['selftext'].str.cat(sep=\" \")\n",
    "lonely1_corpus = lonely1_posts['selftext'].str.cat(sep=\" \")\n",
    "lonely2_corpus = lonely2_posts['selftext'].str.cat(sep=\" \")\n",
    "depr_corpus = depr_posts['selftext'].str.cat(sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes_keywords = {\n",
    "   \"nihilism\": [\"nihil\", \"dark\", \"hopeless\", \"meaning\", \"nothing\", \"empty\", \"death\", \"life\", \"nonesense\", \"bleak\", \"reason\", \"pessimis\", \"worth\"],\n",
    "   \"doom\": [\"doom\", \"collapse\", \"brutal\", \"survive\", \"consciousness\", \"conscience\", \"extinction\", \"clown\", \"climate\", \"ecology\", \"destitute\"],\n",
    "   \"suicide\": [\"suicid\", \"kill\", \"kms\", \"hang\"],\n",
    "   \"alone\": [\"lone\", \"friend\", \"isolated\", \"island\", \"people\", \"presence\", \"nobody\", \"social\", \"bore\", \"boring\"],\n",
    "    \"medical\": [\"medical\", \"mental\", \"health\", \"issue\", \"diagnos\", \"medication\", \"pills\", \"anxiety\", \"psycholog\", \"help\", \"therap\", \"disorder\", \"episode\", \"gender\", \"dysphoria\", \"surgery\", \"mg\", \"lexapro\", \"doctor\", \"neurology\"],\n",
    "    \"relationship\": [\"gf\", \"bf\", \"sex\", \"coom\", \"wank\", \"porn\", \"girl\", \"boy\", \"slut\", \"hore\", \"kiss\", \"edging\", \"virgin\", \"love\", \"masturbat\", \"simp\", \"woman\", \"women\", \"relationship\", \"hug\", \"dating\", \"date\", \"intimacy\", \"girlfriend\", \"boyfriend\", \"fiance\", \"husband\", \"wife\", \"breakup\", \"marriage\", \"divorce\", \"understanding\", \"parent\", \"cheated\"],\n",
    "   \"childhood\": [\"child\", \"abuse\", \"neglect\", \"mom\", \"parents\", \"dad\", \"mother\", \"father\", \"mum\", \"school\", \"course\", \"college\", \"university\", \"grade\", \"teacher\", \"professor\", \"youth\", \"young\", \"recess\", \"class\"],\n",
    "   \"job\": [\"job\", \"work\", \"occupation\", \"career \"],\n",
    "   \"mental health\": [\"mental\", \"depress\", \"diagnos\", \"anxiety\", \"anxious\", \"harm\", \"smile\", \"happy\", \"happi\", \"toxic\", \"emotion\", \"energy\", \"sad\", \"pain\", \"ache\", \"happy\", \"happiness\", \"motivat\", \"unlove\", \"uncared\"],\n",
    "    # \"emotional emptiness\": [\"emotion\", \"energy\", \"sad\", \"pain\", \"ache\", \"happy\", \"happiness\", \"motivat\", \"unloved\", \"unloveable\", \"uncared\"],\n",
    "   \"drugs\": [\"drug\" \"cig\", \"drunk\", \"drink\", \"smoke\", \"weed\", \"alcohol\", \"booze\", \"crack\", \"coke\", \"cocaine\", \"hemp\", \"marijuana\", \"cannabis\", \"intox\"],\n",
    "    \"social issues\": [\"social\", \"shy\", \"timid\", \"talk\", \"support\", \"insecure\", \"rejection\", \"awkward\", \"fit\", \"society\", \"conversation\", \"abandon\", \"extroverted\", \"introverted\", \"unattractive\", \"ugly\"],\n",
    "    \"responsibilities\": [\"responsibl\", \"death\", \"family\", \"pass\", \"job\", \"miscarriage\", \"victim\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = list(themes_keywords.keys())\n",
    "keywords = [themes_keywords[themes] for themes in themes_keywords]\n",
    "themes_keywords_df = pd.DataFrame({\n",
    "    \"themes\": themes,\n",
    "    \"keywords\": keywords\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2311/316709183.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  themes_keywords_latex = themes_keywords_df.to_latex(index=False, float_format=\"%.2f\")\n"
     ]
    }
   ],
   "source": [
    "themes_keywords_latex = themes_keywords_df.to_latex(index=False, float_format=\"%.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Report/Sections/table_keywords1.tex\", \"w\") as f:\n",
    "#     f.write(themes_keywords_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_dict = {\n",
    "    \"themes\": [],\n",
    "    \"re\": [],\n",
    "    \"doomer_matches\": [],\n",
    "    \"lonely1_matches\": [],\n",
    "    \"lonely2_matches\": [],\n",
    "    \"depr_matches\": []\n",
    "}\n",
    "\n",
    "for theme in themes_keywords:\n",
    "    theme_re = re.compile('|'.join(themes_keywords[theme]))\n",
    "    matches_doomer = re.findall(theme_re, doomer_corpus)\n",
    "    matches_lonely1 = re.findall(theme_re, lonely1_corpus)\n",
    "    matches_lonely2 = re.findall(theme_re, lonely2_corpus)\n",
    "    matches_depr = re.findall(theme_re, depr_corpus)\n",
    "\n",
    "    frequencies_dict[\"themes\"].append(theme)\n",
    "    frequencies_dict[\"re\"].append(theme_re)\n",
    "    frequencies_dict[\"doomer_matches\"].append(len(matches_doomer))\n",
    "    frequencies_dict[\"lonely1_matches\"].append(len(matches_lonely1))\n",
    "    frequencies_dict[\"lonely2_matches\"].append(len(matches_lonely2))\n",
    "    frequencies_dict[\"depr_matches\"].append(len(matches_depr))\n",
    "\n",
    "frequencies_df = pd.DataFrame(frequencies_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_doomer_matches = frequencies_df[\"doomer_matches\"].sum()\n",
    "num_lonely1_matches = frequencies_df[\"lonely1_matches\"].sum()\n",
    "num_lonely2_matches = frequencies_df[\"lonely2_matches\"].sum()\n",
    "num_depr_matches = frequencies_df[\"depr_matches\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_doomer_corpus = len(doomer_corpus)\n",
    "len_lonely1_corpus = len(lonely1_corpus)\n",
    "len_lonely2_corpus = len(lonely2_corpus)\n",
    "len_depr_corpus = len(depr_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_df[\"doomer_freqs_tot\"] = frequencies_df[\"doomer_matches\"].div(len_doomer_corpus).mul(100)\n",
    "frequencies_df[\"lonely1_freqs_tot\"] = frequencies_df[\"lonely1_matches\"].div(len_lonely1_corpus).mul(100)\n",
    "frequencies_df[\"lonely2_freqs_tot\"] = frequencies_df[\"lonely2_matches\"].div(len_lonely2_corpus).mul(100)\n",
    "frequencies_df[\"depr_freqs_tot\"] = frequencies_df[\"depr_matches\"].div(len_depr_corpus).mul(100)\n",
    "\n",
    "frequencies_df[\"doomer_freqs_whole\"] = frequencies_df[\"doomer_matches\"].div(num_doomer_matches).mul(100)\n",
    "frequencies_df[\"lonely1_freqs_whole\"] = frequencies_df[\"lonely1_matches\"].div(num_lonely1_matches).mul(100)\n",
    "frequencies_df[\"lonely2_freqs_whole\"] = frequencies_df[\"lonely2_matches\"].div(num_lonely2_matches).mul(100)\n",
    "frequencies_df[\"depr_freqs_whole\"] = frequencies_df[\"depr_matches\"].div(num_depr_matches).mul(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>themes</th>\n",
       "      <th>doomer_freqs_whole</th>\n",
       "      <th>lonely1_freqs_whole</th>\n",
       "      <th>lonely2_freqs_whole</th>\n",
       "      <th>depr_freqs_whole</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nihilism</td>\n",
       "      <td>18.087193</td>\n",
       "      <td>9.533240</td>\n",
       "      <td>9.232299</td>\n",
       "      <td>12.476619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doom</td>\n",
       "      <td>2.607590</td>\n",
       "      <td>0.211236</td>\n",
       "      <td>0.128607</td>\n",
       "      <td>0.282772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suicide</td>\n",
       "      <td>3.868284</td>\n",
       "      <td>3.034122</td>\n",
       "      <td>2.976834</td>\n",
       "      <td>3.982744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alone</td>\n",
       "      <td>16.725212</td>\n",
       "      <td>29.963154</td>\n",
       "      <td>32.343913</td>\n",
       "      <td>14.347703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>medical</td>\n",
       "      <td>5.708683</td>\n",
       "      <td>5.708182</td>\n",
       "      <td>5.524976</td>\n",
       "      <td>11.832982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relationship</td>\n",
       "      <td>12.775036</td>\n",
       "      <td>15.313434</td>\n",
       "      <td>14.239416</td>\n",
       "      <td>10.698114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>childhood</td>\n",
       "      <td>10.294593</td>\n",
       "      <td>8.565873</td>\n",
       "      <td>9.047105</td>\n",
       "      <td>11.577290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>job</td>\n",
       "      <td>5.434993</td>\n",
       "      <td>4.255932</td>\n",
       "      <td>4.101720</td>\n",
       "      <td>6.246182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mental health</td>\n",
       "      <td>10.764390</td>\n",
       "      <td>8.593478</td>\n",
       "      <td>8.357769</td>\n",
       "      <td>16.114243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>drugs</td>\n",
       "      <td>2.150723</td>\n",
       "      <td>0.728525</td>\n",
       "      <td>0.781933</td>\n",
       "      <td>1.209182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>social issues</td>\n",
       "      <td>6.809905</td>\n",
       "      <td>10.752649</td>\n",
       "      <td>9.947357</td>\n",
       "      <td>5.938218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>responsibilities</td>\n",
       "      <td>4.773398</td>\n",
       "      <td>3.340175</td>\n",
       "      <td>3.318072</td>\n",
       "      <td>5.293951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              themes  doomer_freqs_whole  lonely1_freqs_whole  \\\n",
       "0           nihilism           18.087193             9.533240   \n",
       "1               doom            2.607590             0.211236   \n",
       "2            suicide            3.868284             3.034122   \n",
       "3              alone           16.725212            29.963154   \n",
       "4            medical            5.708683             5.708182   \n",
       "5       relationship           12.775036            15.313434   \n",
       "6          childhood           10.294593             8.565873   \n",
       "7                job            5.434993             4.255932   \n",
       "8      mental health           10.764390             8.593478   \n",
       "9              drugs            2.150723             0.728525   \n",
       "10     social issues            6.809905            10.752649   \n",
       "11  responsibilities            4.773398             3.340175   \n",
       "\n",
       "    lonely2_freqs_whole  depr_freqs_whole  \n",
       "0              9.232299         12.476619  \n",
       "1              0.128607          0.282772  \n",
       "2              2.976834          3.982744  \n",
       "3             32.343913         14.347703  \n",
       "4              5.524976         11.832982  \n",
       "5             14.239416         10.698114  \n",
       "6              9.047105         11.577290  \n",
       "7              4.101720          6.246182  \n",
       "8              8.357769         16.114243  \n",
       "9              0.781933          1.209182  \n",
       "10             9.947357          5.938218  \n",
       "11             3.318072          5.293951  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies_df[[\"themes\", \"doomer_freqs_whole\", \"lonely1_freqs_whole\", \"lonely2_freqs_whole\", \"depr_freqs_whole\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using the Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_doomer = list(frequencies_df[\"doomer_freqs_whole\"])\n",
    "comp_lonely1 = list(frequencies_df[\"lonely1_freqs_whole\"])\n",
    "comp_lonely2 = list(frequencies_df[\"lonely2_freqs_whole\"])\n",
    "comp_depr = list(frequencies_df[\"depr_freqs_whole\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(vec1, vec2, emphasize_first=4, emphasize_by=2):\n",
    "    sum = 0\n",
    "    for i in range(len(vec1)):\n",
    "        if(i < emphasize_first):\n",
    "            sum += ((vec1[i] - vec2[i]) ** 2) ** emphasize_by\n",
    "        else:\n",
    "            sum += (vec1[i] - vec2[i]) ** 2\n",
    "    return sqrt(sum)\n",
    "\n",
    "def add_list_vectors(list_vectors):\n",
    "    added_vector = [0 for x in list_vectors[0]]\n",
    "    for vector in list_vectors:\n",
    "        for i in range(len(vector)):\n",
    "            added_vector[i] += vector[i]\n",
    "    return added_vector\n",
    "\n",
    "def scale_vector(vector, scalar):\n",
    "    for i in range(len(vector)):\n",
    "        vector[i] *= scalar\n",
    "    return vector\n",
    "\n",
    "def ave_list_vectors(list_vectors):\n",
    "    return scale_vector(add_list_vectors(list_vectors), (1 / len(list_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct_pred(texts, label_num, re_series, num_categories=4):\n",
    "\n",
    "    # identify the correct and incorrect labels for binary classification\n",
    "    if(label_num == 1 or label_num == 2):\n",
    "        comp_correct = ave_list_vectors([comp_lonely1, comp_lonely2])\n",
    "        comp_incorrect = ave_list_vectors([comp_doomer, comp_depr])\n",
    "    elif(label_num == 0):\n",
    "        comp_correct = comp_doomer\n",
    "        comp_incorrect = ave_list_vectors([comp_lonely1, comp_lonely2, comp_depr])\n",
    "    elif(label_num == 3):\n",
    "        comp_correct = comp_depr\n",
    "        comp_incorrect = ave_list_vectors([comp_lonely1, comp_lonely2, comp_doomer])\n",
    "\n",
    "    # doomer, lonely1, lonely2, depr, correct, incorrect, no_match\n",
    "    predictions = [0 for _ in range(num_categories + 1 + 2)]\n",
    "    lonely1_pred, lonely2_pred = 0, 0\n",
    "\n",
    "    min_num_matches = 1\n",
    "    for text in texts:\n",
    "        comp_vector = []\n",
    "        for theme_re in re_series:\n",
    "            num_matches = len(re.findall(theme_re, text))\n",
    "            comp_vector.append(num_matches)\n",
    "        \n",
    "        # let's not forget to normalise it as a whole of 100%\n",
    "        num_matches = sum(comp_vector)\n",
    "        if(num_matches >= min_num_matches):\n",
    "            for i in range(len(comp_vector)):\n",
    "                comp_vector[i] = (comp_vector[i] / num_matches) * 100\n",
    "\n",
    "            dists = [\n",
    "                get_distance(comp_vector, comp_doomer),\n",
    "                get_distance(comp_vector, comp_lonely1),\n",
    "                get_distance(comp_vector, comp_lonely2),\n",
    "                get_distance(comp_vector, comp_depr)]\n",
    "\n",
    "            shortest_dist_index = dists.index(min(dists))\n",
    "            predictions[shortest_dist_index] += 1\n",
    "\n",
    "            if(get_distance(comp_vector, comp_correct) < get_distance(comp_vector, comp_incorrect)):\n",
    "                # correct binary\n",
    "                predictions[-3] += 1\n",
    "            else:\n",
    "                # incorrect binary\n",
    "                predictions[-2] += 1\n",
    "\n",
    "            if(label_num == 1 or label_num == 2):\n",
    "                if(get_distance(comp_vector, comp_lonely1) < get_distance(comp_vector, comp_lonely2)):\n",
    "                    lonely1_pred += 1\n",
    "                else:\n",
    "                    lonely2_pred += 1\n",
    "        else:\n",
    "            # no match\n",
    "            predictions[-1] += 1\n",
    "\n",
    "    for pred in predictions:\n",
    "        print(pred, ' ', end='')\n",
    "\n",
    "    accuracy = predictions[-3] / (predictions[-3] + predictions[-2])\n",
    "    print(accuracy, ' ', end='')\n",
    "\n",
    "    if(label_num == 1 or label_num == 2):\n",
    "        print(lonely1_pred, ' ', end='')\n",
    "        print(lonely2_pred, ' ', end='')\n",
    "        if(label_num == 1):\n",
    "            print(lonely1_pred / (lonely1_pred + lonely2_pred), ' ', end='')\n",
    "        else:\n",
    "            print(lonely2_pred / (lonely1_pred + lonely2_pred), ' ', end='')\n",
    "\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doomer lonely1 lonely2 depr correct incorrect no_match binary accuracy lonely1 lonely2\n",
      "1213  291  466  1031  1948  1053  15  0.6491169610129957  \n",
      "712  860  2330  1076  3264  1714  11  0.6556850140618722  2648  2330  0.5319405383688228  \n",
      "336  533  1496  437  2059  743  3  0.7348322626695217  1306  1496  0.5339043540328337  \n",
      "2230  867  1059  4834  6632  2358  13  0.7377085650723025  \n"
     ]
    }
   ],
   "source": [
    "min_length = 0\n",
    "print(\"doomer\", \"lonely1\", \"lonely2\", \"depr\", \"correct\", \"incorrect\", \"no_match\", \"binary accuracy\", \"lonely1\", \"lonely2\")\n",
    "get_num_correct_pred(doomer_posts[doomer_posts[\"text_len\"] >= min_length][\"selftext\"], 0, frequencies_df[\"re\"])\n",
    "get_num_correct_pred(lonely1_posts[lonely1_posts[\"text_len\"] >= min_length][\"selftext\"], 1, frequencies_df[\"re\"])\n",
    "get_num_correct_pred(lonely2_posts[lonely2_posts[\"text_len\"] >= min_length][\"selftext\"], 2, frequencies_df[\"re\"])\n",
    "get_num_correct_pred(depr_posts[depr_posts[\"text_len\"] >= min_length][\"selftext\"], 3, frequencies_df[\"re\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "          themes &  doomer\\_freqs\\_whole &  lonely1\\_freqs\\_whole &  lonely2\\_freqs\\_whole &  depr\\_freqs\\_whole \\\\\n",
      "\\midrule\n",
      "        nihilism &               18.09 &                 9.53 &                 9.23 &             12.48 \\\\\n",
      "            doom &                2.61 &                 0.21 &                 0.13 &              0.28 \\\\\n",
      "         suicide &                3.87 &                 3.03 &                 2.98 &              3.98 \\\\\n",
      "           alone &               16.73 &                29.96 &                32.34 &             14.35 \\\\\n",
      "         medical &                5.71 &                 5.71 &                 5.52 &             11.83 \\\\\n",
      "    relationship &               12.78 &                15.31 &                14.24 &             10.70 \\\\\n",
      "       childhood &               10.29 &                 8.57 &                 9.05 &             11.58 \\\\\n",
      "             job &                5.43 &                 4.26 &                 4.10 &              6.25 \\\\\n",
      "   mental health &               10.76 &                 8.59 &                 8.36 &             16.11 \\\\\n",
      "           drugs &                2.15 &                 0.73 &                 0.78 &              1.21 \\\\\n",
      "   social issues &                6.81 &                10.75 &                 9.95 &              5.94 \\\\\n",
      "responsibilities &                4.77 &                 3.34 &                 3.32 &              5.29 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2311/3115676395.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  frequencies_latex = frequencies_df[[\"themes\", \"doomer_freqs_whole\", \"lonely1_freqs_whole\", \"lonely2_freqs_whole\", \"depr_freqs_whole\"]].to_latex(index=False, float_format=\"%.2f\")\n"
     ]
    }
   ],
   "source": [
    "frequencies_latex = frequencies_df[[\"themes\", \"doomer_freqs_whole\", \"lonely1_freqs_whole\", \"lonely2_freqs_whole\", \"depr_freqs_whole\"]].to_latex(index=False, float_format=\"%.2f\")\n",
    "print(frequencies_latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
